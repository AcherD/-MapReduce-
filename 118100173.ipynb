{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# all import\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mushroom data cleaning\n",
    "This part clean the data and spilt the data for using."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial samples: 5644\n",
      "number of features: 22\n",
      "number of class labels: 2\n",
      "\n",
      "Class Labels are: ['e' 'p']\n",
      "\n",
      "Take a look at unique outcomes per feature\n",
      "0th: ['b' 'c' 'f' 'k' 's' 'x']\n",
      "1th: ['f' 'g' 's' 'y']\n",
      "2th: ['b' 'c' 'e' 'g' 'n' 'p' 'w' 'y']\n",
      "3th: ['f' 't']\n",
      "4th: ['a' 'c' 'f' 'l' 'm' 'n' 'p']\n",
      "5th: ['a' 'f']\n",
      "6th: ['c' 'w']\n",
      "7th: ['b' 'n']\n",
      "8th: ['g' 'h' 'k' 'n' 'p' 'r' 'u' 'w' 'y']\n",
      "9th: ['e' 't']\n",
      "10th: ['b' 'c' 'e' 'r']\n",
      "11th: ['f' 'k' 's' 'y']\n",
      "12th: ['f' 'k' 's' 'y']\n",
      "13th: ['b' 'c' 'g' 'n' 'p' 'w' 'y']\n",
      "14th: ['b' 'c' 'g' 'n' 'p' 'w' 'y']\n",
      "15th: ['p']\n",
      "16th: ['w' 'y']\n",
      "17th: ['n' 'o' 't']\n",
      "18th: ['e' 'l' 'n' 'p']\n",
      "19th: ['h' 'k' 'n' 'r' 'u' 'w']\n",
      "20th: ['a' 'c' 'n' 's' 'v' 'y']\n",
      "21th: ['d' 'g' 'l' 'm' 'p' 'u']\n",
      "\n",
      "Remove stalk-root feature because it has some missing data\n",
      "Remove veil-type feature because it is always 'p'\n",
      "\n",
      "After removing the two features\n",
      "number of features: 20\n",
      "\n",
      "number of training samples: 4233\n",
      "number of test samples: 1411\n"
     ]
    }
   ],
   "source": [
    "# Mushroom Data Set\n",
    "# https://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "# here the path might be different when the file dir have changed\n",
    "data = pd.read_csv('Downloads/mushrooms.csv', delimiter=',', header=0)\n",
    "#exclude any training example that has missing values for stalk root\n",
    "data = data[data['stalk-root'] != '?']\n",
    "\n",
    "#first we must do a bit of data inspection and cleaning\n",
    "#y is the first col of the data\n",
    "y = data.iloc[:,0]\n",
    "#X is the data except the first col\n",
    "X = data.iloc[:,1:]\n",
    "#Cn is the class name, here we actually have 2 class e and p\n",
    "Cn=len(np.unique(y))\n",
    "\n",
    "n,d = X.shape\n",
    "\n",
    "print (\"initial samples: {}\".format(n))\n",
    "print (\"number of features: {}\".format(d))\n",
    "print (\"number of class labels: {}\".format(Cn))\n",
    "print ()\n",
    "\n",
    "print (\"Class Labels are: {}\".format(np.unique(y)))\n",
    "print ()\n",
    "\n",
    "print (\"Take a look at unique outcomes per feature\")\n",
    "for i in range(0,d):\n",
    "\tprint (\"{}th: {}\".format(i,np.unique(X.iloc[:,i])))\n",
    "\n",
    "print ()\n",
    "print (\"Remove stalk-root feature because it has some missing data\")\n",
    "print (\"Remove veil-type feature because it is always 'p'\")\n",
    "#X = np.delete(X,(10,15),axis=1)\n",
    "X = X.drop(labels=\"stalk-root\",axis=1)\n",
    "X = X.drop(labels=\"veil-type\",axis=1)\n",
    "\n",
    "n,d = X.shape\n",
    "# dictionary master list of unique features\n",
    "featureDict = {}\n",
    "for i in range(0,d):\n",
    "\tfeatureDict[i]= np.unique(X.iloc[:,i])\n",
    "\n",
    "print ()\n",
    "print (\"After removing the two features\")\n",
    "print (\"number of features: {}\".format(d))\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "\n",
    "print ()\n",
    "print (\"number of training samples: {}\".format(n_train))\n",
    "print (\"number of test samples: {}\".format(n_test))\n",
    "\n",
    "# Isolate the training set based on classification label\n",
    "X_train_e = X_train[y_train=='e']\n",
    "X_train_p = X_train[y_train=='p']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Sequence MapReduce"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def seq_map(col):\n",
    "    #print(type(col))\n",
    "    resultDict = {}\n",
    "#  put all attribute to the list\n",
    "    attributeList = list(np.unique(col))\n",
    "#  for every attribute calculate the total number\n",
    "    for attribute in attributeList:\n",
    "        sumValue = col.isin([attribute]).sum()\n",
    "        resultDict[attribute] = sumValue\n",
    "    return pd.Series(resultDict)\n",
    "\n",
    "def seq_reduce(result,mappedData):\n",
    "    #print(type(mappedData))\n",
    "    attribute = mappedData[0]\n",
    "    #\n",
    "    #sumValues = mappedData[1][mappedData[1].notnull()].sum(axis = 0)\n",
    "    #attributeValue = mappedData[1][mappedData[1].notnull()] / sumValues\n",
    "\n",
    "    # calculate the probability P(Attribute | Class)\n",
    "    attributeValue = mappedData[1][mappedData[1].notnull()] / len(mappedData)\n",
    "    result[attribute] = attributeValue\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "def seq_MapReduce(data):\n",
    "    # print(data.apply(seq_map, axis=0))\n",
    "    return pd.DataFrame(reduce(seq_reduce,data.apply(seq_map, axis=0).items(),{}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "\n",
    "#testing block\n",
    "#a = X_train_e.apply(seq_map, axis=0)\n",
    "#print(pd.DataFrame(reduce(seq_reduce,a.items(),{})))\n",
    "#print(seq_MapReduce(X_train_e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ParallelMapReduce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def mapper_run(data):\n",
    "         result = data.apply(seq_map,axis=0)\n",
    "         return result\n",
    "\n",
    "def reduce_run(data):\n",
    "         result = reduce(seq_reduce, data.items(),{})\n",
    "         return result\n",
    "\n",
    "def get_result(data):\n",
    "    results = pd.DataFrame(data[0])\n",
    "    for item in range(1, len(data)):\n",
    "        results = pd.concat([results,pd.DataFrame(data[item])],axis=1)\n",
    "    return results\n",
    "\n",
    "class ParallelMapReduce(object):\n",
    "    def __init__(self, map_func, reduce_func, num_workers=None):\n",
    "        self.num_workers = num_workers\n",
    "        self.map_func = map_func\n",
    "        self.reduce_func = reduce_func\n",
    "        self.pool = multiprocessing.Pool(num_workers)\n",
    "\n",
    "    def partition(self, n, iterable):\n",
    "        i = iter(iterable)\n",
    "        piece = list(islice(i, n))\n",
    "        while piece:\n",
    "            yield piece\n",
    "            piece = list(islice(i, n))\n",
    "\n",
    "    # def spilt_data(self,data):\n",
    "    #     i = iter(data.columns.values)\n",
    "    #     piece = data[i]\n",
    "    #     while piece:\n",
    "    #         yield piece\n",
    "    #         piece = data[i]\n",
    "    def mapper_run(self,data):\n",
    "         result = data.apply(self.map_func,axis=0)\n",
    "         return result\n",
    "\n",
    "    def reduce_run(self,data):\n",
    "         result = reduce(self.reduce_func, data.items(),{})\n",
    "         return result\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # print(type(inputs))\n",
    "        # -------\n",
    "        spilt_data = np.array_split(inputs,self.num_workers,axis = 1)\n",
    "        spilt_data = pd.Series(spilt_data)\n",
    "        # print(\"**********\")\n",
    "        # print(type(spilt_data))\n",
    "        # -------\n",
    "        values = self.pool.imap(mapper_run,spilt_data)\n",
    "        # values = self.pool.map(self.map_func,spilt_data)\n",
    "        #print ('>>> MAPPED VALUES (%s values): %s, ...' % (len(values), str(values[:10])))\n",
    "\n",
    "        # deal with data again\n",
    "        values = pd.concat(values,axis=1)\n",
    "        # print(\"----------\")\n",
    "        # print(type(values))\n",
    "        # print(values)\n",
    "        # spilt data\n",
    "        values = np.array_split(values,self.num_workers,axis=1)\n",
    "        # print(type(values))\n",
    "        # values = pd.DataFrame(values)\n",
    "        # print(type(values))\n",
    "\n",
    "        # values = self.pool.map(self.reduce_func({},values.items()),values)\n",
    "        values = self.pool.map(reduce_run,values)\n",
    "        # values = self.pool.map(reduce(self.reduce_func,values.items(),{}),values)\n",
    "        values = get_result(values)\n",
    "        # print ('>>> REDUCED VALUES', values)\n",
    "\n",
    "        return values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# test block\n",
    "#i = iter(X_train_e.columns.values)\n",
    "#print(next(i))\n",
    "#print(X_train_e[\"cap-shape\"])\n",
    "# mapreduce = ParallelMapReduce(seq_map, seq_reduce, 10)\n",
    "# print (mapreduce(X_train_e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Map reduce the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "#train from data in sequence version\n",
    "trained_e = seq_MapReduce(X_train_e)\n",
    "trained_p = seq_MapReduce(X_train_p)\n",
    "trained_X = seq_MapReduce(X_train)\n",
    "trained_y = y_train.value_counts()\n",
    "trained_e.loc[\"r\"] = 1\n",
    "trained_p.loc[\"r\"] = 1\n",
    "#trained_y = seq_MapReduce(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# -----------here is the parallel version train---------------------------------------------\n",
    "# mapreduce = ParallelMapReduce(seq_map, seq_reduce, 10)\n",
    "# trained_e = mapreduce(X_train_e)\n",
    "# trained_p = mapreduce(X_train_p)\n",
    "# trained_X = mapreduce(X_train)\n",
    "# trained_e.loc[\"r\"] = 1\n",
    "# trained_p.loc[\"r\"] = 1\n",
    "# --------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Classifier\n",
    "This classifier using the MAP rules to guss the class of the target mushroom"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# probability dictionary in data frame of the class P\n",
    "def probability_of_P(row):\n",
    "    probabilityP = trained_y[\"p\"] / len(y_train)\n",
    "    probabilityResult = 1\n",
    "    for item in row.items():\n",
    "        probabilityResult = probabilityResult * trained_p[item[0]].loc[item[1]]\n",
    "    probabilityResult = probabilityP * probabilityResult\n",
    "    if np.isnan(probabilityResult):\n",
    "        probabilityResult = 0\n",
    "    #print(probabilityResult)\n",
    "    return probabilityResult\n",
    "#X_test.apply(probability_of_P,axis=1)\n",
    "\n",
    "# probability dictionary in data frame of the class E\n",
    "def probability_of_E(row):\n",
    "    probabilityE = trained_y[\"e\"] / len(y_train)\n",
    "    probabilityResult = 1\n",
    "    for item in row.items():\n",
    "        probabilityResult = probabilityResult * trained_e[item[0]].loc[item[1]]\n",
    "    probabilityResult = probabilityE * probabilityResult\n",
    "    if np.isnan(probabilityResult):\n",
    "        probabilityResult = 0\n",
    "    #print(probabilityResult)\n",
    "    return probabilityResult\n",
    "#X_test.apply(probability_of_E,axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# compare the probability of both\n",
    "def classifier(row):\n",
    "    probabilityP = probability_of_P(row)\n",
    "    probabilityE = probability_of_E(row)\n",
    "    #print(probabilityP)\n",
    "    # if probabilityP higher than probabilityE, make the decision p\n",
    "    if probabilityP > probabilityE:\n",
    "        result = pd.Series([\"p\"])\n",
    "        return result\n",
    "    # else, make the decision e\n",
    "    else:\n",
    "        result = pd.Series([\"e\"])\n",
    "        return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generation of test results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability of correct prediction:\n",
      "99.08%\n"
     ]
    }
   ],
   "source": [
    "test_result_1 = X_test.apply(classifier,axis=1)\n",
    "resultDict = dict(test_result_1[0])\n",
    "correctDict = dict(y_test)\n",
    "#print(dict(test_result_1[0]))\n",
    "#print(dict(y_test))\n",
    "#print(len(resultDict.items()& correctDict.items()))\n",
    "print ()\n",
    "print (\"Probability of correct prediction:\")\n",
    "print (\"{:.2%}\".format(len(resultDict.items()& correctDict.items()) / len(correctDict)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-bef8ded6",
   "language": "python",
   "display_name": "PyCharm (pythonProject)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}